{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0e53cf4",
   "metadata": {},
   "source": [
    "# Más desarrollos de modelos, empleando más funcionalidades de Scikit-Learn\n",
    "> Dataset disponible en https://www.kaggle.com/datasets/ziya07/college-student-management-dataset\n",
    "\n",
    "Este dataset incluye múltiples características relacionadas con el rendimiento estudiantil, métricas de uso de plataformas de aprendizaje, donde la variable objetivo es el riesgo de abandono. Comencemos con la acción."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1391fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv(\"./datasets/college_student_management_data.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5faf3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2957145",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ba0c58",
   "metadata": {},
   "source": [
    "Las variables están completas, por lo que no se requiere de imputación por valores faltantes. A continuación, se visualizará la distribución de las variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b92b153",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=['student_id'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45fe131d",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_columns = df.select_dtypes(\"number\").columns\n",
    "numerical_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ac2e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns = df.select_dtypes(\"object\").columns\n",
    "categorical_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789e3cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histograms for each numerical column\n",
    "df[numerical_columns].hist(figsize=(18, 12), bins=20, layout=(2, 5))\n",
    "plt.suptitle('Histograms of Numerical Columns', fontsize=16)\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "plt.show()\n",
    "\n",
    "# Box plots for each numerical column\n",
    "fig, axes = plt.subplots(2, 5, figsize=(18, 10))\n",
    "for ax, col in zip(axes.flatten(), numerical_columns):\n",
    "    df.boxplot(column=col, ax=ax)\n",
    "    ax.set_title(col)\n",
    "plt.suptitle('Box Plots of Numerical Columns', fontsize=16)\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c106de70",
   "metadata": {},
   "source": [
    "No se presentan outliers univariados en ninguna de las variables encontradas. Las variables tampoco parecen seguir una distribución estadística clara como la normal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2308771",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(18, 10))\n",
    "for ax, col in zip(axes.flatten(), categorical_columns):\n",
    "    df[col].value_counts().plot(kind='bar', ax=ax)\n",
    "    ax.set_xticklabels(ax.get_xticklabels(), rotation=45)\n",
    "    ax.set_title(col)\n",
    "    ax.set_ylabel('Counts')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e65a20",
   "metadata": {},
   "source": [
    "La clase `risk_level` está claramente desbalanceada. Se usará creación de datos sintéticos para balancear las clases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be6a47f",
   "metadata": {},
   "source": [
    "# Preprocesamiento\n",
    "Se convertirá la variable objetivo a una numérica ordinal mediante LabelEncoder, y las demás variables predictoras serán transformadas mediante OneHotEncoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b4323c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "\n",
    "categorical_columns = categorical_columns.drop(['risk_level'])\n",
    "\n",
    "ohe = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "le = LabelEncoder()\n",
    "\n",
    "# Se elimina la columna 'risk_level' de X y se aplica OneHotEncoder a las demás columnas categóricas, manteniendo la estructura del dataframe.\n",
    "X = df.copy().drop(columns=['risk_level'])\n",
    "X.drop(columns=categorical_columns, inplace=True)\n",
    "X = pd.concat([X, pd.DataFrame(ohe.fit_transform(df[categorical_columns]), columns=ohe.get_feature_names_out(categorical_columns))], axis=1)\n",
    "X.head()\n",
    "\n",
    "y = le.fit_transform(df['risk_level'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d69b81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(pd.DataFrame(X).head())\n",
    "display(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56dec7ee",
   "metadata": {},
   "source": [
    "A continuación, a partir de X, se crearán dos versiones: Una normalizada y otra escalada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d006eb63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "std = StandardScaler()\n",
    "mm = MinMaxScaler()\n",
    "\n",
    "X_std = X.copy()\n",
    "X_std[numerical_columns] = std.fit_transform(X[numerical_columns])\n",
    "X_mm = pd.DataFrame(mm.fit_transform(X), columns=X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40830b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(X_std.head())\n",
    "display(X_mm.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf18577",
   "metadata": {},
   "source": [
    "# Sobre la detección de outliers multivariados\n",
    "\n",
    "Este es un campo que aún no exploro, y la idea es aprender y probar metodologías.\n",
    "\n",
    "De buenas a primeras se me ocurre que mediante técnicas de aprendizaje no supervisado como K-means se pueden encontrar outliers, donde estos se encontrarían a una distancia grande de los centroides. No obstante, también hay otros métodos de aprendizaje no supervisado, como DBSCAN, que es de clustering, o IsolationForest. \n",
    "\n",
    "**DBSCAN** consiste en identificar **grupos de puntos que estén muy cerca entre sí** (o regiones con alta densidad), marcando como ruido o outliers las regiones que están poco denamente pobladas, con el beneficio de que los datos **no tienen que estar linealmente separados**; ni siquiera que tengan una forma específica [ver imagen de ejemplo](https://upload.wikimedia.org/wikipedia/commons/thumb/0/05/DBSCAN-density-data.svg/1024px-DBSCAN-density-data.svg.png). En contraparte, asume que los grupos a encontrar tienen la misma cantidad de observaciones, y puede tener problemas al encontrar clusters con densidades diferentes, [para lo cual fue creado HDBSCAN](https://scikit-learn.org/stable/modules/clustering.html#hdbscan).\n",
    "\n",
    "**Isolation Forest** es un algoritmo basado en árboles de decisión. Asumiendo que los **outliers son pocos y diferentes de la muestra**, se construyen múltiples **árboles seleccionando aleatoriamente una característica**, y eligiendo un umbral de acuerdo con el dominio de aplicación. El valor de corte sirve para dividir los datos y generar un mínimo y máximo, los cuales serán divididos recursivamente hasta (1.) **aislar cada instancia en una hoja del árbol**, o (2.) **alcanzar una profundidad máxima**. De esta manera, los outliers requerirán de menos hojas para quedar aislados, siendo esta la técnica para identificarlos.\n",
    "\n",
    "---\n",
    "\n",
    "En este ejercicio, me interesa evaluar el rendimiento de la creación de múltiples modelos empleando DBSCAN, IsolationForest, o sin aplicar ningún tipo de detección de outliers, con el fin de evaluar cómo se ve afectado el entrenamiento por estos datos. Para ello, se utilizarán múltiples *Pipelines* de Scikit-Learn, donde la configuración de los modelos de machine learning para realizar la clasificación serán comunes entre todos los pipelines, y lo que variará será el tratamiento de preprocesamiento dado a los datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121c9d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "dbscan = DBSCAN(eps=0.7, min_samples=2).fit(X_mm, y)\n",
    "labels = dbscan.labels_\n",
    "\n",
    "# Number of clusters in labels, ignoring noise if present.\n",
    "n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "n_noise_ = list(labels).count(-1)\n",
    "\n",
    "print(\"Estimated number of clusters: %d\" % n_clusters_)\n",
    "print(\"Estimated number of noise points: %d\" % n_noise_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b0b483",
   "metadata": {},
   "source": [
    "Aparentemente, DBSCAN detecta todos los datos como ruido bajo estos ajustes. En una primera instancia, me sugiere que todos los datos están homogéneamente distribuidos, pero no tendría sentido, dado que implicaría que no hay forma de distinguir las clases entre sí. Otra interpretación es que la distancia promedio entre los puntos es mayor a 0.7, para lo cual un análisis mediante el método del codo puede ser revelador. Se usará KNN con $n = 5$ para graficar las distancias al 4to vecino, y elegir un valor de inflexión."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7173e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "neigh = NearestNeighbors(n_neighbors=5)\n",
    "neigh.fit(X_mm)\n",
    "dists, _ = neigh.kneighbors(X_mm)\n",
    "k_dist = np.sort(dists[:, -1])  # distancia al 4.º vecino\n",
    "plt.plot(k_dist)\n",
    "plt.ylabel(\"4th nearest neighbor distance\")\n",
    "plt.xlabel(\"Points sorted by distance\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a216abee",
   "metadata": {},
   "source": [
    "Los valores _adecuados_ estarían en el intervalo $\\text{eps} \\in [1.1, ~1.2]$, donde se produce el codo superior. Probando con la cota inferior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d51760",
   "metadata": {},
   "outputs": [],
   "source": [
    "dbscan = DBSCAN(eps=1.1, min_samples=5).fit(X_mm, y)\n",
    "labels = dbscan.labels_\n",
    "\n",
    "# Number of clusters in labels, ignoring noise if present.\n",
    "n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "n_noise_ = list(labels).count(-1)\n",
    "\n",
    "print(\"Estimated number of clusters: %d\" % n_clusters_)\n",
    "print(\"Estimated number of noise points: %d\" % n_noise_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc04449d",
   "metadata": {},
   "source": [
    "Y con la cota superior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de997f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "dbscan = DBSCAN(eps=1.2, min_samples=5).fit(X_mm, y)\n",
    "labels = dbscan.labels_\n",
    "\n",
    "# Number of clusters in labels, ignoring noise if present.\n",
    "n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "n_noise_ = list(labels).count(-1)\n",
    "\n",
    "print(\"Estimated number of clusters: %d\" % n_clusters_)\n",
    "print(\"Estimated number of noise points: %d\" % n_noise_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc64730",
   "metadata": {},
   "source": [
    "En ningún caso la cantida de outliers es elevada, donde para la cota inferior el porcentaje de puntos ruidosos es menor al 1.4%.\n",
    "\n",
    "Como parte de lo mencionado anteriormente, resulta interesante comparar estos resultados con IsolationForest. No obstante, no sé muy bien cómo hacer esto. Sin ajustar hiperparámetros todas las observaciones son categorizadas como outliers. Tampoco conozco de ninguna heurística o método para ajustar el parámetro `contamination`, así que lo único que se me ocurre es usar el porcentaje de outliers detectados en el paso anterior.\n",
    "\n",
    "Dado el porcentaje de outliers detectado por DBSCAN a raíz del valor `eps` hallado de manera empírica por el método de KNN, tomo la decisión de no eliminarlos. Son un porcentaje bastante ínfimo que no considero que pueda ensuciar los modelos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b8d85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import IsolationForest\n",
    "iso_forest = IsolationForest(contamination=0.015, random_state=42)\n",
    "iso_forest.fit(X_mm)\n",
    "outliers = iso_forest.predict(X_mm)\n",
    "outliers = np.where(outliers == -1, 1, 0)  # Convert -1 to 1 (outlier) and 1 to 0 (inlier)\n",
    "# Count the number of outliers\n",
    "n_outliers = np.sum(outliers)\n",
    "print(f\"Number of outliers detected: {n_outliers}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29570669",
   "metadata": {},
   "source": [
    "# Balanceo de clases\n",
    "\n",
    "Principalmente, `imbalanced-learning` implementa métodos métodos estadísticos y basados en machine learning. En este ejercicio, me interesa realizar over-sampling de las clases de la variable objetivo con menos observaciones (séase `medium` y `low`). De los métodos estadísticos destacan SMOTE y ADASYN, las cuales interpolan los datos ya existentes para generar nuevas muestras. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4579861a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X_mm, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7450bbd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(pd.DataFrame(X_resampled).info())\n",
    "display(y_resampled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf081bb2",
   "metadata": {},
   "source": [
    "# Selección de características\n",
    "\n",
    "De nuevo, me basaré en las metodologías disponibles en scikit-learn. Como métodos univariados, se pueden seleccionar las K características con mejor puntaje, o aquellas características que superen un percentil de puntaje. Este función de puntaje se deja a elección, y para este notebook será `mutual_info_classif`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b6c195",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest,  SelectPercentile,mutual_info_classif\n",
    "\n",
    "kbest = SelectKBest(score_func=mutual_info_classif, k=7).fit(X_resampled, y_resampled)\n",
    "X_kbest = kbest.transform(X_resampled)\n",
    "print(\"Selected features based on mutual information:\")\n",
    "selected_features = X_kbest.shape[1]\n",
    "print(f\"Number of selected features: {selected_features}\")\n",
    "selected_feature_names = X.columns[kbest.get_support()]\n",
    "print(f\"Selected feature names: {selected_feature_names.tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "298bdbf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "top70 = SelectPercentile(score_func=mutual_info_classif, percentile=70).fit(X_resampled, y_resampled)\n",
    "X_top70 = top70.transform(X_resampled)\n",
    "print(\"Selected features based on 70th percentile of mutual information:\")\n",
    "selected_features_70 = X_top70.shape[1]\n",
    "print(f\"Number of selected features: {selected_features_70}\")\n",
    "selected_feature_names_70 = X.columns[top70.get_support()]\n",
    "print(f\"Selected feature names: {selected_feature_names_70.tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f425bb72",
   "metadata": {},
   "source": [
    "Los resultados comparten algunas características en común. En todo caso, es posible explicar el dataset con menos variables de las originales, al menos desde una perspectiva univariada. Las perspectivas multivariadas, según he leído, se basan en análisis de rendimiento de modelos a medida de que se quitan características. Para entrenar modelos ya estoy yo, así que me quedaré con las top 7 variables y con eso entrenaré algunos modelos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12461102",
   "metadata": {},
   "source": [
    "# Entrenamiento\n",
    "\n",
    "Me gustan los modelos populares, así que iré con perceptron multicapas, máquinas de soporte vectorial, procesos gaussianos, y quizás alguno como bagging o boosting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f17d88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e60d9bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f448a348",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "mlp_grid = [\n",
    "    {\n",
    "        'hidden_layer_sizes': [(50,), (100,), (50, 50), (25, 25, 25)],\n",
    "        'activation': ['relu', 'logistic'],\n",
    "        'alpha': np.linspace(0.0001, 0.01, 5),\n",
    "        'learning_rate': ['constant', 'adaptive'],\n",
    "        'solver': ['adam'],\n",
    "        'max_iter': [200]\n",
    "    }\n",
    "]\n",
    "\n",
    "mlp_cv = GridSearchCV(MLPClassifier(random_state=42), mlp_grid, scoring='f1_weighted', cv=5, n_jobs=-1, verbose=2)\n",
    "mlp_cv.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9dc59b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(f\"Best estimator: {mlp_cv.best_estimator_}\")\n",
    "display(f\"Best score: {mlp_cv.best_score_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "947841f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_best_estimator = mlp_cv.best_estimator_\n",
    "y_pred = mlp_best_estimator.predict(X_test)\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=le.classes_))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6713a12",
   "metadata": {},
   "source": [
    "probando por algún motivo con tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d56131e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "svm_grid = [\n",
    "    {\n",
    "        'C': np.logspace(-3, 3, 7),\n",
    "        'kernel': ['linear'],\n",
    "        'max_iter': [1000]\n",
    "    },\n",
    "    {\n",
    "        'C': np.logspace(-3, 3, 7),\n",
    "        'kernel': ['rbf'],\n",
    "        'gamma': ['scale', 'auto'],\n",
    "        'max_iter': [1000]\n",
    "    },\n",
    "    {\n",
    "        'C': np.logspace(-3, 3, 7),\n",
    "        'kernel': ['poly'],\n",
    "        'degree': [2, 3, 4, 5, 6],\n",
    "        'gamma': ['scale', 'auto'],\n",
    "        'max_iter': [1000]\n",
    "    }\n",
    "\n",
    "]\n",
    "\n",
    "svm_cv = GridSearchCV(SVC(random_state=42), svm_grid, scoring='f1_weighted', cv=5, n_jobs=-1, verbose=2)\n",
    "svm_cv.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c61e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(f\"Best estimator: {svm_cv.best_estimator_}\")\n",
    "display(f\"Best score: {svm_cv.best_score_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c9b7c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_best_estimator = svm_cv.best_estimator_\n",
    "y_pred = svm_best_estimator.predict(X_test)\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=le.classes_))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15214f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "\n",
    "hgb_grid = [\n",
    "    {\n",
    "        'max_iter': [200],\n",
    "        'max_depth': [3, 5, 7],\n",
    "        'learning_rate': np.linspace(0.0001, 0.1, 5),\n",
    "        'l2_regularization': [0.0, 0.1, 0.2],\n",
    "        'max_leaf_nodes': [None, 10, 20],\n",
    "        'min_samples_leaf': [1, 5, 10],\n",
    "        'max_bins': [255],\n",
    "    }\n",
    "]\n",
    "hgb_cv = GridSearchCV(HistGradientBoostingClassifier(random_state=42), hgb_grid, scoring='f1_weighted', cv=5, n_jobs=-1, verbose=2)\n",
    "hgb_cv.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4295704f",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(f\"Best estimator: {hgb_cv.best_estimator_}\")\n",
    "display(f\"Best score: {hgb_cv.best_score_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b414975",
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_best_estimator = svm_cv.best_estimator_\n",
    "y_pred = svm_best_estimator.predict(X_test)\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=le.classes_))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
